{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGI405 - Text Classification with Feature Selection and Grid Search\n",
    "\n",
    "See the [README](README.md) for further notes on this notebook (e.g. installing required libraries if you are not using the class JupyterHub). See the [CHANGELOG](CHANGELOG.md) for version number and a history of changes.  \n",
    "\n",
    "What can text classification techniques tell us about sentiment or tone? Can text classification help us find distinguishing features between two groups of texts?  \n",
    "\n",
    "This notebook introduces you to:\n",
    "\n",
    "1. A new data-set relevant to sentiment classification and using the Huggingface Datasets library.  \n",
    "2. Feature selection using [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "3. Automating parameter tuning using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "**Remember:** Each time you change settings below, you will need to rerun the cells that create the pipeline and does the classification.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task:</strong> Throughout the notebook there are defined tasks for you to do. Watch out for them - they will have a box around them like this! Make sure you take some notes as you go.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Below we are importing required libraries. We will be using [scikit-learn](https://scikit-learn.org) for text classification in DIGI405. We will use the Naive Bayes Classifier. Scikit-learn has different feature extraction methods based on counts or tf-idf weights. We will also use NLTK for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words_sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from textplumber.preprocess import NLTKPreprocessor\n",
    "from textplumber.tokens import TokensVectorizer\n",
    "from textplumber.core import get_stop_words\n",
    "from textplumber.report import preview_row_text, plot_confusion_matrix, preview_pipeline_features, preview_dataset, preview_split_by_label_column\n",
    "from textplumber.store import TextFeatureStore\n",
    "\n",
    "import warnings\n",
    "\n",
    "# in the interests of readability, ignoring this warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Your stop_words may be inconsistent with your preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_sklearn = list(stop_words_sklearn)\n",
    "stop_words_nltk = get_stop_words(save_to = 'stop_words_nltk.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_binary_display_features(pipeline, label_names, features_to_show=20):\n",
    "\tvect = pipeline.named_steps['vectorizer']\n",
    "\tclf = pipeline.named_steps['classifier']\n",
    "\tfeature_names = vect.get_feature_names_out()\n",
    "\tlogodds=clf.feature_log_prob_[1]-clf.feature_log_prob_[0]\n",
    "\n",
    "\t# if selector in pipeline\n",
    "\tif 'selector' in pipeline.named_steps:\n",
    "\t\t# Get SelectKBest feature scores\n",
    "\t\tfeatures = pipeline.named_steps['selector']\n",
    "\t\t# get top k feature indices\n",
    "\t\tcols = features.get_support(indices=True)\n",
    "\t\t# get corresponding feature scores\n",
    "\t\ttop_k_feature_scores = [features.scores_[i] for i in cols if i in cols]\n",
    "\t\tfeature_names = [feature_names[i] for i in cols]\n",
    "\n",
    "\tdf = pd.DataFrame({\n",
    "\t\t'Feature': feature_names,\n",
    "\t\t'Log-Odds': logodds,\n",
    "\t})\n",
    "\n",
    "\tif 'selector' in pipeline.named_steps:\n",
    "\t\t# if scoring func is mi \n",
    "\t\tif pipeline.named_steps['selector'].score_func == mutual_info_classif:\n",
    "\t\t\tscore_column_name = 'MI Score'\n",
    "\t\telse:\n",
    "\t\t\tscore_column_name = 'Feature Score'\n",
    "\n",
    "\t\tdf[score_column_name] = top_k_feature_scores\n",
    "\n",
    "\tif 'selector' in pipeline.named_steps:\n",
    "\t\tprint('Top features by information gain')\n",
    "\t\tprint('================================')\n",
    "\t\tsorted_df = df.sort_values([score_column_name], ascending=False).head(features_to_show)\n",
    "\t\tdisplay(sorted_df)\n",
    "\n",
    "\tprint(\"Features most indicative of\", label_names[0])\n",
    "\tprint('============================' + '='*len(label_names[0]))\n",
    "\n",
    "\tsorted_df = df.sort_values('Log-Odds', ascending=True).head(features_to_show)\n",
    "\tdisplay(sorted_df)\n",
    "\n",
    "\tprint(\"Features most indicative of\", label_names[1])\n",
    "\tprint('============================' + '='*len(label_names[1]))\n",
    "\n",
    "\tsorted_df = df.sort_values('Log-Odds', ascending=False).head(features_to_show)\n",
    "\tdisplay(sorted_df)\n",
    "\n",
    "def get_feature_frequencies(pipeline, text):\n",
    "\tpreprocessor = Pipeline(pipeline.steps[:-1])\n",
    "\tfrequency = preprocessor.transform([text]).toarray()[0].T\n",
    "\tfeature_names = preprocessor.named_steps['vectorizer'].get_feature_names_out()\n",
    "\t\n",
    "\tif 'selector' in pipeline.named_steps:\n",
    "\t\tcols = pipeline.named_steps['selector'].get_support(indices=True)\n",
    "\t\tfeature_names = [feature_names[i] for i in cols]\n",
    "\n",
    "\tdf = pd.DataFrame(frequency, index=feature_names, columns=['frequency'])\n",
    "\tdf = df[df['frequency'] > 0].sort_values('frequency', ascending=False)\n",
    "\tif len(df) < 1:\n",
    "\t\treturn 'No features extracted from this document.'\n",
    "\telse:\n",
    "\t\treturn df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus and set train/test split\n",
    "\n",
    "Today we will work with a movie reviews data-set. The reviews are annotated with sentiment polarities \"pos\" and \"neg\". The Sentiment Polarity Dataset Version 2.0 is distributed with NLTK, but we are using it here to introduce Huggingface's [Datasets library](https://huggingface.co/docs/datasets/en/index). We will use the datasets library to load the data. The Huggingface website has a [dataset page](https://huggingface.co/datasets/polsci/sentiment-polarity-dataset-v2.0) with more information, a preview, including links and citation information from the creators of the data-set. Take a look now to help understand the data-set and how it was created.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('polsci/sentiment-polarity-dataset-v2.0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using a Textplumber function to get a summary of splits, features and the number of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the features for our train split shows the names of the features and the label names. Notice the field called label has type ClassLabel, which means it is defined as a label. Not all HuggingFace datasets will have this defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we preview our train split as a dataframe we can see that the label is stored as a numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a count for each label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_split_by_label_column(dataset, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(dataset['train']['text'])\n",
    "y = np.array(dataset['train']['label'])\n",
    "\n",
    "label_names = dataset['train'].features['label'].names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell also sets the train/test split. 80% of the data is used for training and 20% is used for testing. The documents are assigned to each group randomly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect documents and labels\n",
    "\n",
    "In the next cells we can look at the data we have imported. Firstly, we will preview the document labels and a brief excerpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the labels and text into a dataframe\n",
    "df = pd.DataFrame(list(zip(y_train, X_train)), columns =['label', 'text'])\n",
    "# using the class_names for the labels\n",
    "df['label'] = df['label'].apply(lambda x: label_names[x])\n",
    "\n",
    "# setting the display width to show more of the text - change this to see more or less\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "# showing the first 10 rows\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this cell to inspect a specific document and its label based on its index in the training set. Note: The indexes will change each time you import the data above because of the random train/test split.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 1:</strong> Inspect some off the documents in each class and think about the kinds of words that might be useful features in this text classification task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = 1 # Change this to the the index of the document you want to preview\n",
    "preview_row_text(df, train_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define preprocessing and feature extraction settings\n",
    "\n",
    "You can consult the [text classification introduction notebook](https://github.com/polsci/text-classification-introduction) for more information on each setting below.\n",
    "\n",
    "On the first run through, just use these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = None\n",
    "lowercase = True\n",
    "min_token_length = 0\n",
    "remove_punctuation = True\n",
    "remove_numbers = False\n",
    "stop_word_list = 'nltk'\n",
    "extra_stop_words = []\n",
    "min_df = 0.0\n",
    "max_df = 1.0\n",
    "vectorizer_type = 'count' # either 'count' or 'tfidf'\n",
    "max_features = 1000\n",
    "ngram_range = (1, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New settings for the feature selection step\n",
    "\n",
    "The feature selection step selects the top features based on [univariate statistical tests](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). Here we are using [mutual information scores](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) to assess the dependency between each feature and the class labels. \n",
    "\n",
    "The value below sets the number of features to select and use in our classifier. In this case we will start with 100 features, based on the mutual information score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a pipeline: feature extraction → feature selection → classifier\n",
    "\n",
    "This is similar to the Scikit-learn pipeline we setup in the introductory notebook, but there is a new pipeline component for feature selection prior to training the classifier. The TokenVectorizer class takes a tokenized text as input and outputs either Tf-idf weights or counts depending on how you set it above.\n",
    "\n",
    "**Important Note 1:** When you change settings above or reload your dataset you should rerun these cells again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the feature store is used to reduce preprocessing time after the first run\n",
    "feature_store = TextFeatureStore('text-classification-feature-selection.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare stop words\n",
    "if stop_word_list == 'nltk':\n",
    "    stop_words = stop_words_nltk\n",
    "elif stop_word_list == 'sklearn':\n",
    "    stop_words = stop_words_sklearn\n",
    "else:\n",
    "    stop_words = []\n",
    "\n",
    "if len(extra_stop_words) > 0:\n",
    "\tstop_words = stop_words + extra_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you shouldn't need to change anything in this cell!\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', NLTKPreprocessor(feature_store = feature_store)),\n",
    "    ('vectorizer', TokensVectorizer(feature_store = feature_store,\n",
    "                                   vectorizer_type = vectorizer_type,\n",
    "\t\t\t\t\t\t\t\t   lowercase = lowercase,\n",
    "\t\t\t\t\t\t\t\t   min_token_length = min_token_length,\t\n",
    "\t\t\t\t\t\t\t\t   remove_punctuation = remove_punctuation,\t\n",
    "\t\t\t\t\t\t\t\t   remove_numbers = remove_numbers,\n",
    "\t\t\t\t\t\t\t\t   stop_words = stop_words, \n",
    "                                   min_df = min_df,\n",
    "\t\t\t\t\t\t\t\t   max_df = max_df,\n",
    "\t\t\t\t\t\t\t\t   max_features = max_features,\n",
    "\t\t\t\t\t\t\t\t   ngram_range = ngram_range,\n",
    "                                   normalizer = normalizer\n",
    "                                   )),\n",
    "    ('selector', SelectKBest(score_func = mutual_info_classif, k=kbest)),\n",
    "    ('classifier', MultinomialNB()), #here is where you would specify an alternative classifier\n",
    "])\n",
    "\n",
    "display(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note 2:** This cell outputs the settings you used above, which you can cut and paste into a document to keep track of changes you are making and their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you shouldn't need to change anything in this cell!\n",
    "\n",
    "print('Classifier settings')\n",
    "print('===================')\n",
    "print('Classes:', label_names)\n",
    "print()\n",
    "print('Pipeline Components')\n",
    "for i, step in enumerate(pipeline.named_steps):\n",
    "    print(f'\\tStep {i + 1}: {pipeline.named_steps[step].__class__.__name__}')\n",
    "\n",
    "print()\n",
    "\n",
    "print('vectorizer_type:', vectorizer_type)\n",
    "print()\n",
    "\n",
    "print('normalizer:', normalizer)\n",
    "print('lowercase:', lowercase)\n",
    "print('stop_word_list:', stop_word_list)\n",
    "print('extra_stop_words:', extra_stop_words)\n",
    "print('min_token_length:', min_token_length)\n",
    "print('remove_punctuation:', remove_punctuation)\n",
    "print('remove_numbers:', remove_numbers)\n",
    "\n",
    "print()\n",
    "\n",
    "print('min_df:', min_df)\n",
    "print('max_df:', max_df)\n",
    "print('max_features:', max_features)\n",
    "print('ngram_range:', ngram_range)\n",
    "\n",
    "print()\n",
    "\n",
    "print('kbest:', kbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier and predict labels on test data\n",
    "\n",
    "Because we are adding the feature selection step, the classifier will be slower as it has to calculate MI scores for each feature and rank them. This will increase the more features you extract.\n",
    "\n",
    "**Important Note:** You can cut and paste the model output into a document (with the settings above) to keep track of changes you are making and their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you shouldn't need to change anything in this cell!\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_predicted = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the evaluation metrics on the held-out data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print report\n",
    "print(metrics.classification_report(y_test, y_predicted, target_names = label_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the correct and incorrect predictions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test = y_test, y_predicted = y_predicted, target_classes = [0, 1], target_names = label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the features ranked by information gain (MI) and by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_show = 10\n",
    "nb_binary_display_features(pipeline, label_names, features_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all features\n",
    "\n",
    "You can inspect how the text data moves through the pipeline below, including which token features were output by the vectorizer, and which features were selected based on information gain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preview_pipeline_features(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect correctly/incorrectly classified documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe from y_predicted, y_test and the text\n",
    "predictions_df = pd.DataFrame(data = {'true': y_test, 'predicted': y_predicted})\n",
    "predictions_df['predicted'] = predictions_df['predicted'].apply(lambda x: label_names[x])\n",
    "predictions_df['true'] = predictions_df['true'].apply(lambda x: label_names[x])\n",
    "predictions_df['correct'] = predictions_df['true'] == predictions_df['predicted']\n",
    "predictions_df['text'] = X_test\n",
    "\n",
    "# output a preview of docs for each cell of confusion matrix ...\n",
    "for true_target, target_name in enumerate(label_names):\n",
    "    for predicted_target, target_name in enumerate(label_names):\n",
    "        if true_target == predicted_target:\n",
    "            print(f'\\nCORRECTLY CLASSIFIED: {label_names[true_target]}')\n",
    "        else:\n",
    "            print(f'\\n{label_names[true_target]} INCORRECTLY CLASSIFIED as: {label_names[predicted_target]}')\n",
    "        print('=================================================================')\n",
    "\n",
    "        display(predictions_df[(predictions_df['true'] == label_names[true_target]) & (predictions_df['predicted'] == label_names[predicted_target])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 2:</strong> Inspect documents that were correct and incorrectly classified. Why are some documents incorrectly classified?\n",
    "</div>\n",
    "\n",
    "## Preview document and its features\n",
    "\n",
    "Use this cell to preview a document using its index in the test set. You can see the predicted label, its actual label, the full text and the features for this specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 11 # preview a text from the cell above using its index\n",
    "\n",
    "preview_row_text(predictions_df, test_id)\n",
    "\n",
    "print('Features')\n",
    "print('========')\n",
    "\n",
    "print(get_feature_frequencies(pipeline, X_test[test_id]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 3:</strong> Try changing the tokenisation to include punctuation. What punctuation emerges as useful features? How are these punctuation features being used?\n",
    "</div>    \n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 4:</strong>    Now increase the number of most frequent tokens to allow the feature selection step to inspect and score lots more less frequent words. What number of frequent tokens improves the features that can be identified? \n",
    "</div>    \n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 5:</strong>    After you’ve identified the best settings to improve the performance metrics of the classifier, review the incorrectly classified documents again. Identify any unexpected word features and identify whether they may be true indicators of sentiment, or just coincidence.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Parameter Tuning\n",
    "\n",
    "In the remainder of the lab we will work through automated parameter tuning. Warning: this may take some time!\n",
    "\n",
    "## Define the Pipeline for the Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you shouldn't need to change anything in this cell!\n",
    "\n",
    "pipeline_for_gridsearch = Pipeline([\n",
    "    ('preprocessor', NLTKPreprocessor(feature_store = feature_store)),\n",
    "    ('vectorizer', TokensVectorizer(feature_store = feature_store,\n",
    "                                   vectorizer_type = vectorizer_type,\n",
    "\t\t\t\t\t\t\t\t   lowercase = lowercase,\n",
    "\t\t\t\t\t\t\t\t   min_token_length = min_token_length,\t\n",
    "\t\t\t\t\t\t\t\t   remove_punctuation = remove_punctuation,\t\n",
    "\t\t\t\t\t\t\t\t   remove_numbers = remove_numbers,\n",
    "\t\t\t\t\t\t\t\t   stop_words = stop_words, \n",
    "                                   min_df = min_df,\n",
    "\t\t\t\t\t\t\t\t   max_df = max_df,\n",
    "\t\t\t\t\t\t\t\t   max_features = max_features,\n",
    "\t\t\t\t\t\t\t\t   ngram_range = ngram_range,\n",
    "                                   normalizer = normalizer\n",
    "                                   )),\n",
    "    ('selector', SelectKBest(score_func = mutual_info_classif, k=kbest)),\n",
    "    ('classifier', MultinomialNB()), #here is where you would specify an alternative classifier\n",
    "])\n",
    "\n",
    "display(pipeline_for_gridsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Search Space\n",
    "\n",
    "This step is to define the space of parameters and estimators we want to search through. We do this in the form of a dictionary and we use double underscore notation (__) to refer to the parameters of different steps in our pipeline. We will be trying out different values of k for the feature selector SelectKBest. However, this is not an exhaustive list of parameters that we can search. We could search parameters for the feature extraction steps as well. Some example parameters are commented out and you could test with them, but note that for every line you uncomment you are increasing the number of combinations and the time it will take to run the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{#'vectorizer__normalizer'  : [None, 'SnowballStemmer'],\n",
    "                 #'normalizer__stop_word_list'  : [None, stop_words_nltk],\n",
    "\t\t\t\t #'vectorizer__vectorizer_type' : ['count', 'tfidf'],\n",
    "                 #'vectorizer__min_df' : [0.0, 0.01, 0.5],\n",
    "                 #'vectorizer__max_df' : [0.5, 0.75, 1.0],\n",
    "                 #'vectorizer__max_features' : range(1000, 5001, 1000), # this starts at 1000 and ends at 5000 with steps of 1000\n",
    "                 #'vectorizer__ngram_range' : [(1, 1), (1, 2)],\n",
    "                 'selector__k'              : range(50, 701, 50), #this starts at 50 and ends at 700 with steps of 50\n",
    "                 #'selector__score_func'     : [mutual_info_classif, chi2],\n",
    "                 #'classifier': [MultinomialNB(), LogisticRegression(max_iter=1000)]\n",
    "                }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scorers can be either be one of the predefined metric strings or a scorer callable, like the one returned by make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'Accuracy': make_scorer(accuracy_score)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the GridSearch \n",
    "\n",
    "This is where the computation happens! We will now pass our pipeline into GridSearchCV to test our search space (of feature preprocessing, feature selection, model selection, and hyperparameter tuning combinations) using cross-validation with 3-folds. If we had more time, we would probably increase the number of folds (cv) to 5 or 10.\n",
    "\n",
    "Setting refit='Accuracy', refits an estimator on the whole dataset with the parameter setting that has the best cross-validated Accuracy score.\n",
    "That estimator is made available at ``gridsearch.best_estimator_`` along with parameters like ``gridsearch.best_score_``, ``gridsearch.best_params_`` and ``gridsearch.best_index_``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(estimator = pipeline_for_gridsearch, \n",
    "                    param_grid         = search_space, \n",
    "                    scoring            = scoring,\n",
    "                    cv                 = 3, \n",
    "                    refit              = 'Accuracy',\n",
    "                    return_train_score = True,\n",
    "                    verbose            = 3)\n",
    "\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Results\n",
    "\n",
    "We can access the best result of our search using the best_estimator_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gridsearch.best_estimator_)\n",
    "display(gridsearch.best_params_)\n",
    "display(gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting from the cv_results_ dictionary\n",
    "\n",
    "Demonstrating methods of extracting values from the cv_results_ dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = gridsearch.cv_results_['mean_test_Accuracy']\n",
    "stds  = gridsearch.cv_results_['std_test_Accuracy']\n",
    "\n",
    "for mean, std, params in zip(means, stds, gridsearch.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_best_predicted = gridsearch.best_estimator_.predict(X_test)\n",
    "\n",
    "# print report\n",
    "print(metrics.classification_report(y_test, y_best_predicted, target_names = label_names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test = y_test, y_predicted = y_best_predicted, target_classes = [0, 1], target_names = label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the results\n",
    "\n",
    "Plot code taken from https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "\n",
    "Note: that the x and y axis range is set below - you may need to change this depending on values you chose above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our results\n",
    "results = gridsearch.cv_results_\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.title(\"GridSearchCV evaluating parameters using the Accuracy scorer.\",\n",
    "          fontsize=16)\n",
    "\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# adjust these according to your accuracy results and range values.\n",
    "ax.set_xlim(0, 700)\n",
    "ax.set_ylim(0.600, 1)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results['param_selector__k'].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(sorted(scoring), ['b']):\n",
    "    for sample, style in (('train', '--'), ('test', '-')):\n",
    "        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                        sample_score_mean + sample_score_std,\n",
    "                        alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "        ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                alpha=1 if sample == 'test' else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "    best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.3f with k=%s\" % (best_score, X_axis[best_index]),\n",
    "                (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got to this point in the lab, try changing search_space above to search more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digi405-23S1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
